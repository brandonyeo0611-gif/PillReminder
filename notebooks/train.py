"""
train.py
=========
Trains AngleLSTMNet on your labeled keypoint CSV files.

USAGE:
  python3 notebooks/train.py

REQUIRES:
  datasets/train_action_pose_keypoint.csv
  datasets/test_action_pose_keypoint.csv
  (generated by scripts/extract_keypoints.py)
"""

import sys, os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import numpy as np

from src.classification_keypoint import (
    AngleLSTMNet, load_and_preprocess,
    LABELS, NUM_ANGLE_FEATURES, SEQUENCE_LENGTH
)

# â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRAIN_CSV    = "datasets/train_action_pose_keypoint.csv"
TEST_CSV     = "datasets/test_action_pose_keypoint.csv"
SAVE_PATH    = "model/trained_carewatch.pt"
LABEL_PATH   = "model/label_classes.txt"

HIDDEN_SIZE  = 128
NUM_LAYERS   = 3
LSTM_DROPOUT = 0.3
FC_DROPOUT   = 0.5
BATCH_SIZE   = 64
LR           = 0.001
WEIGHT_DECAY = 1e-5
NUM_EPOCHS   = 150
PATIENCE     = 15

# â”€â”€ SETUP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
device = torch.device("mps" if torch.backends.mps.is_available() else
                      "cuda" if torch.cuda.is_available() else "cpu")
print(f"Device: {device}")

os.makedirs("model", exist_ok=True)

# â”€â”€ DATA â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
X_train, X_test, y_train, y_test, le = load_and_preprocess(TRAIN_CSV, TEST_CSV, SEQUENCE_LENGTH)

X_train_t = torch.tensor(X_train)
X_test_t  = torch.tensor(X_test)
y_train_t = torch.tensor(y_train)
y_test_t  = torch.tensor(y_test)

train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=BATCH_SIZE, shuffle=True)
test_loader  = DataLoader(TensorDataset(X_test_t,  y_test_t),  batch_size=BATCH_SIZE)

# â”€â”€ MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
num_classes = len(le.classes_)
model = AngleLSTMNet(
    input_size=NUM_ANGLE_FEATURES,
    hidden_size=HIDDEN_SIZE,
    num_layers=NUM_LAYERS,
    num_classes=num_classes,
    lstm_dropout=LSTM_DROPOUT,
    fc_dropout=FC_DROPOUT,
).to(device)

optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)
criterion = nn.CrossEntropyLoss()
scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)

# â”€â”€ TRAINING LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
best_val_loss = float("inf")
patience_counter = 0

for epoch in range(1, NUM_EPOCHS + 1):
    # Train
    model.train()
    train_loss, train_correct, train_total = 0.0, 0, 0
    for X_batch, y_batch in train_loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        logits = model(X_batch)
        loss = criterion(logits, y_batch)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
        preds = logits.argmax(dim=1)
        train_correct += (preds == y_batch).sum().item()
        train_total   += y_batch.size(0)

    # Validate
    model.eval()
    val_loss, val_correct, val_total = 0.0, 0, 0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            logits = model(X_batch)
            loss   = criterion(logits, y_batch)
            val_loss    += loss.item()
            preds = logits.argmax(dim=1)
            val_correct += (preds == y_batch).sum().item()
            val_total   += y_batch.size(0)

    avg_train_loss = train_loss / len(train_loader)
    avg_val_loss   = val_loss   / len(test_loader)
    train_acc      = 100 * train_correct / train_total
    val_acc        = 100 * val_correct   / val_total

    scheduler.step(avg_val_loss)

    print(f"Epoch {epoch:3d}/{NUM_EPOCHS}  "
          f"train_loss={avg_train_loss:.4f}  train_acc={train_acc:.1f}%  "
          f"val_loss={avg_val_loss:.4f}  val_acc={val_acc:.1f}%")

    # Save best
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        torch.save(model.state_dict(), SAVE_PATH)
        # Save label classes so inference knows the mapping
        with open(LABEL_PATH, "w") as f:
            f.write("\n".join(le.classes_))
        print(f"  ðŸ’¾ Best model saved (val_loss={best_val_loss:.4f})")
    else:
        patience_counter += 1
        if patience_counter >= PATIENCE:
            print(f"Early stopping at epoch {epoch}")
            break

print(f"\nâœ… Training complete. Best val_loss: {best_val_loss:.4f}")
print(f"   Model saved to {SAVE_PATH}")