import sys
import cv2
import numpy as np
from pydantic import BaseModel
import torch
from ultralytics import YOLO
from ultralytics.engine.results import Results

class GetKeypoint(BaseModel):
    NOSE:           int = 0
    LEFT_SHOULDER:  int = 5
    RIGHT_SHOULDER: int = 6
    LEFT_ELBOW:     int = 7
    RIGHT_ELBOW:    int = 8
    LEFT_WRIST:     int = 9
    RIGHT_WRIST:    int = 10


class DetectPerson:
    def __init__(self, detection_model='yolo11n.pt'):
        self.device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
        self.detection_model = detection_model
        self.__load_model()

    def __load_model(self):
        self.model = YOLO(model=self.detection_model).to(self.device)

    def __call__(self, image: np.ndarray) -> Results:
        results = self.model.predict(image, conf=0.25, classes=[0])[0]  # class 0 is person
        return results

class DetectKeypoint:
    def __init__(self, pose_model='yolo11x-pose.pt'):
        self.device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
        self.pose_model = pose_model
        self.get_keypoint = GetKeypoint()
        self.__load_model()

    def __load_model(self):
        self.model = YOLO(model=self.pose_model).to(self.device)

    def extract_keypoint(self, keypoint: np.ndarray) -> dict:
        extracted = {}
        for name, idx in self.get_keypoint.__fields__.items():
            idx_value = getattr(self.get_keypoint, name)

            if idx_value < len(keypoint):
                extracted[name.lower()] = {
                    'xy': keypoint[idx_value][:2],
                    'confidence': keypoint[idx_value][2] if len(keypoint[idx_value]) > 2 else None
                }
        return extracted

    def get_all_keypoints(self, results: Results) -> list:
        all_keypoints = []
        if results.keypoints is not None:
            keypoints_data = results.keypoints.data.cpu().numpy() if torch.is_tensor(results.keypoints.data) else results.keypoints.data

            for person_keypoints in keypoints_data:
                keypoint_data = self.extract_keypoint(person_keypoints)
                all_keypoints.append(keypoint_data)
        return all_keypoints

    def get_xy_keypoint(self, results: Results) -> np.ndarray:
        all_keypoints = self.get_all_keypoints(results)
        if not all_keypoints:
            return np.array([])

        xy_points = []
        for person in all_keypoints:
            person_points = []
            for _, point_data in person.items():
                person_points.extend(point_data['xy'])
            xy_points.append(person_points)

        return np.array(xy_points)

    def __call__(self, image: np.ndarray) -> Results:
        results = self.model.predict(image, save=False)[0]
        return results

class AngleFeatureGenerator:
    def __init__(self):
        self.get_keypoint = GetKeypoint()

    def calculate_angle(self, p1: np.ndarray, p2: np.ndarray, p3: np.ndarray) -> float:
        if any(point is None or not isinstance(point, np.ndarray) or len(point) != 2 for point in [p1, p2, p3]):
            return None

        ba = p1 - p2
        bc = p3 - p2

        if np.all(ba == 0) or np.all(bc == 0):
            return None

        cosine = np.dot(ba, bc) / (np.linalg.norm(ba) * np.linalg.norm(bc))
        angle = np.degrees(np.arccos(np.clip(cosine, -1.0, 1.0)))
        return angle

    def calculate_vector_angle(self, vector: np.ndarray) -> float:
        if vector is None or not isinstance(vector, np.ndarray) or len(vector) != 2:
            return None

        if np.all(vector == 0):
            return None

        angle = np.degrees(np.arctan2(vector[1], vector[0]))
        return angle

    def generate_features(self, keypoints: dict) -> dict:
        features = {}

        def get_coords(key):
            point_data = keypoints.get(key.lower(), {})
            coords = point_data.get('xy')
            return np.array(coords) if coords is not None else None

        # Calculate all angles as in the original code
        features['right_elbow_angle'] = self.calculate_angle(
            get_coords('RIGHT_SHOULDER'),
            get_coords('RIGHT_ELBOW'),
            get_coords('RIGHT_WRIST')
        )

        features['left_elbow_angle'] = self.calculate_angle(
            get_coords('LEFT_SHOULDER'),
            get_coords('LEFT_ELBOW'),
            get_coords('LEFT_WRIST')
        )


        # ... [same angle calculations as before]

        return features

    def process_keypoints(self, keypoints_list: list) -> list:
        processed_features = []
        for keypoints in keypoints_list:
            features = self.generate_features(keypoints)
            features = {k: v if v is not None else 0.0 for k, v in features.items()}
            processed_features.append(features)
        return processed_features

class CombinedDetector:
    def __init__(self, detection_model_path='model/yolo11n.pt',
                 pose_model_path='model/yolo11x-pose.pt'):
        self.person_detector = DetectPerson(detection_model_path)
        self.keypoint_detector = DetectKeypoint(pose_model_path)
        self.feature_generator = AngleFeatureGenerator()

    def process_frame(self, frame: np.ndarray) -> tuple:
        # Detect persons
        person_results = self.person_detector(frame)
        person_boxes = person_results.boxes

        # Detect poses
        pose_results = self.keypoint_detector(frame)
        keypoints_list = self.keypoint_detector.get_all_keypoints(pose_results)

        # Generate angle features
        angle_features = self.feature_generator.process_keypoints(keypoints_list)

        return person_boxes, keypoints_list, angle_features

def visualize_results(image: np.ndarray, person_boxes, keypoints_list: list, angle_features: list) -> np.ndarray:
    """Visualize detected persons, poses, and angles on the image"""
    img_copy = image.copy()

    # Draw person detection boxes
    boxes_xyxy = person_boxes.xyxy.cpu().numpy()
    conf_scores = person_boxes.conf.cpu().numpy()

    for box, conf in zip(boxes_xyxy, conf_scores):
        x1, y1, x2, y2 = map(int, box)
        cv2.rectangle(img_copy, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(img_copy, f'Person: {conf:.2f}',
                    (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Draw keypoints and connections
    for keypoints in keypoints_list:
        for name, point_data in keypoints.items():
            point = tuple(map(int, point_data['xy']))
            cv2.circle(img_copy, point, 3, (255, 0, 0), -1)

            # Draw connections between keypoints
            connections = [
                ('left_shoulder', 'right_shoulder'),
                ('left_shoulder', 'left_elbow'),
                ('right_shoulder', 'right_elbow'),
                ('left_elbow', 'left_wrist'),
                ('right_elbow', 'right_wrist')
            ]

            for start_point, end_point in connections:
                if start_point in keypoints and end_point in keypoints:
                    pt1 = tuple(map(int, keypoints[start_point]['xy']))
                    pt2 = tuple(map(int, keypoints[end_point]['xy']))
                    cv2.line(img_copy, pt1, pt2, (0, 255, 255), 2)

    # Draw angle features
    y_offset = 30
    for i, features in enumerate(angle_features):
        for name, value in features.items():
            if value is not None:
                text = f"Person {i+1} {name}: {value:.1f}Â°"
                cv2.putText(img_copy, text, (10, y_offset),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
                y_offset += 20

    return img_copy

if __name__ == "__main__":
    # Initialize detector
    detector = CombinedDetector()

    # Process video or image
    video_path = '../datasets/20210804_145925_deepfaked.mp4'
    cap = cv2.VideoCapture(video_path)

    try:
        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break

            # Process frame
            person_boxes, keypoints_list, angle_features = detector.process_frame(frame)

            # Visualize results
            annotated_frame = visualize_results(frame, person_boxes, keypoints_list, angle_features)

            # Display frame
            cv2.imshow('Combined Detection and Pose Estimation', annotated_frame)

            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        cap.release()
        cv2.destroyAllWindows()